---
layout: page
title: Publications
permalink: /publications/
---

My publications can also be found on \[[Google Scholar Citations\]](https://scholar.google.com/citations?user=aI_16pYAAAAJ&hl=en), \[[DBLP\]](https://dblp.uni-trier.de/pers/hd/k/Kucherenko:Taras), and \[[ResearchGate\]](https://www.researchgate.net/profile/Taras_Kucherenko). 

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />
<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">


### 2020

* **Taras Kucherenko**, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje Henter. *The GENEA Challenge 2020: Benchmarking gesture-generation systems on common data.* International Workshop on Generation and Evaluation of Non-Verbal Behaviour for Embodied Agents. 2020 \[[Paper\]](https://zenodo.org/record/4094697) \[[Video\]](https://youtu.be/Y-5dgBQk34c) \[[Project Page\]](https://genea-workshop.github.io/2020/#gesture-generation-challenge)

* **Taras  Kucherenko**,  Dai  Hasegawa, Naoshi Kaneko, Gustav  Eje  Henter, and Hedvig Kjellström.
*Moving fast and slow: Analysis of representations and post-processing in speech-driven automatic gesture generation.*
arxiv preprint. 2020
\[[Paper\]](https://www.researchgate.net/publication/343096046_Moving_fast_and_slow_Analysis_of_representations_and_post-processing_in_speech-driven_automatic_gesture_generation) \[[Code\]](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder) \[[Video\]](https://youtu.be/Iv7UBe92zrw) \[[Project Page\]](../_posts/2020-01-14-Audio2Gestures.md)

* **Taras Kucherenko**, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, Simon Alexanderson, Iolanda Leite, and Hedvig Kjellström. *Gesticulator: A framework for semantically-aware speech-driven gesture generation*. International Conference on Multimodal Interaction (ICMI '20). 2020. \[[Paper\]](../papers/Gesticulator_ICMI_2020.pdf) \[[Code\]](https://github.com/svito-zar/Gesticulator) \[[Video\]](https://youtu.be/VQ8he6jjW08) \[[Project Page\]](../_posts/2020-08-03-Gesticulator.md) <span style="font-size: 11px;" class="badge badge-info mb-2">Best Paper Award<i class="fas fa-award"></i></span>

* Patrik Jonell^, **Taras Kucherenko**^, Ilaria Torre, Jonas Beskow. *Can we trust online crowdworkers? Comparing online and offline participants in a preference test of virtual agents.* International Conference on Intelligent Virtual Agents (IVA'20). 2020 \[[Paper\]](../papers/Can_we_trust_online_crowd_workers_2020.pdf) \[[Video\]](https://youtu.be/OSuOvolaI6Y)

* Patrik Jonell, **Taras Kucherenko**, Gustav Eje Henter, Jonas Beskow. *Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings*. International Conference on Intelligent Virtual Agents (IVA'20). 2020. \[[Paper\]](https://raw.githubusercontent.com/jonepatr/lets_face_it/master/paper/jonell_lets_face_it.pdf) \[[Code\]](https://github.com/jonepatr/lets_face_it) \[[Video\]](https://youtu.be/RhazMS4L_bk) \[[Project Page\]](https://jonepatr.github.io/lets_face_it/) <span style="font-size: 11px;" class="badge badge-info mb-2">Best Paper Award<i class="fas fa-award"></i></span>

* Simon Alexanderson, Éva Székely, Gustav Eje Henter, **Taras Kucherenko**, and Jonas Beskow.
*Generating coherent spontaneous speech and gesture from text.* International Conference on Intelligent Virtual Agents (IVA'20). 2020. \[[Paper\]](../papers/Generating_speech_n_gestures_from_text_2020.pdf) \[[Project Page\]](https://simonalexanderson.github.io/IVA2020)

* Simon Alexanderson, Gustav  Eje  Henter, **Taras Kucherenko**,  and Jonas Beskow. *Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows*. Computer Graphics Forum. 2020. (EuroGraphics 2020 <span style="font-size: 11px;" class="badge badge-info mb-2">Honourable Mention Award <i class="fas fa-award"></i></span>) \[[Paper\]](../papers/alexanderson2020style.pdf) \[[Code\]](https://github.com/simonalexanderson/StyleGestures) \[[Video\]](https://youtu.be/egf3tjbWBQE)

### 2019
* Pieter Wolfert, **Taras Kucherenko**, Hedvig Kjellström, Tony Belpaeme. *Should Beat Gestures Be Learned Or Designed? A Benchmarking User Study.* ICDL-EPIROB 2019 Workshop on Naturalistic Non-Verbal and Affective Human-Robot Interactions, Oslo, August 19, 2019 \[[Paper\]](https://pieterwolfert.com/files/epirob_camera_final.pdf) \[[Code\]](https://github.com/Svito-zar/Speech_driven_gesture_generation) \[[Poster\]](../posters/should_gesture_be_learned_poster.pdf)

* Patrik Jonell, **Taras Kucherenko**, Erik Ekstedt, Jonas Beskow. *Learning Non-verbal Behavior for a Social Robot from YouTube Videos.* ICDL-EPIROB 2019 Workshop on Naturalistic Non-Verbal and Affective Human-Robot Interactions, Oslo, August 19, 2019 \[[Paper\]](../papers/learning_non-verbal_behavio(ICDL-EPIROB2019).pdf) \[[Code\]](https://github.com/jonepatr/glow-non-verbal-robot-behavior) \[[Poster\]](../posters/Jonel_2019_final_ICDL_poster.pdf)

* **Taras  Kucherenko**,  Dai  Hasegawa, Gustav  Eje  Henter, Naoshi  Kaneko, and Hedvig Kjellström.
*Analyzing input and output representations for speech-driven gesture generation.*
International Conference on Intelligent Virtual Agents (IVA '19), Paris, July 02–05, 2019
\[[Paper\]](../papers/Aud2Repr2Pose_Kucherenko_2019.pdf) \[[Code\]](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder) \[[Video\]](https://youtu.be/Iv7UBe92zrw) \[[bib\]](https://people.kth.se/~ghe/pubs/bib/kucherenko2019analyzing.bib) \[[Project Page\]](../_posts/2020-01-14-Audio2Gestures.md)

* **Taras  Kucherenko**,  Dai  Hasegawa,  Naoshi  Kaneko,  Gustav  Eje  Henter, and Hedvig Kjellström. 
*On the importance of representations for speech-driven gesture generation.*
18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS '19), Extended Abstract,
Montreal, May 13–17, 2019 \[[Paper\]](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p2072.pdf) \[[Poster\]](https://www.researchgate.net/publication/333148799_On_the_Importance_of_Representations_for_Speech-Driven_Gesture_Generation) \[[bib\]](https://people.kth.se/~ghe/pubs/bib/kucherenko2019importance.bib) \[[Project Page\]](../_posts/2020-01-14-Audio2Gestures.md)



### 2018

* **Taras  Kucherenko**. 
*Data driven non-verbal behavior generation for humanoid robots.*
International Conference on Multimodal Interaction (ICMI '18), Doctoral Consortium,
Boulder, Oct 12-17, 2018 \[[Paper\]](https://dl.acm.org/citation.cfm?doid=3242969.3264970)

* **Taras  Kucherenko**, Jonas Beskow and Hedvig Kjellström. 
*A neural network approach to missing marker reconstruction in human motion capture.*
arXiv preprint (2018) \[[Paper\]](https://www.researchgate.net/publication/323626902_A_Neural_Network_Approach_to_Missing_Marker_Reconstruction_in_Human_Motion_Capture) \[[Code\]](https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction) \[[Video\]](https://youtu.be/mi75gzEhbHI) 


### 2017

* Patrik Jonell, Joseph Mendelson, Thomas Storskog, Goran Hagman, Per Ostberg, Iolanda Leite, **Taras Kucherenko**, Olga Mikheeva, Ulrika Akenine, Vesna Jelic, Alina Solomon, Jonas Beskow, Joakim Gustafson, Miia Kivipelto, Hedvig Kjellstrom. *Machine Learning and Social Robotics for Detecting Early Signs of Dementia*
arXiv preprint (2017) \[[Paper\]](https://arxiv.org/abs/1709.01613)

* **Taras  Kucherenko** and Hedvig Kjellström. *Towards Context-Preserving Human to Robot Motion Mapping.* The First Swedish Symposium on Deep Learning, Stockholm, 2017 \[[Paper\]](https://www.csc.kth.se/~hedvig/publications/ssdl_17.pdf)


## ACM open-access links

<div class="multi-search multi-search--issue-item">
    <ul class="rlist ">
        <li class="grid-item separated-block--dashed--bottom">
            <div class="issue-item clearfix">
                <div class="issue-item__citation">
                    <div class="issue-heading">research-article</div>
                </div>
                <div class="issue-item__content">
                    <h5 class="issue-item__title"><a
                            href="https://dl.acm.org/doi/10.1145/3383652.3423911?cid=99659309831">Let's
                            Face It: Probabilistic Multi-modal
                            Interlocutor-aware Generation of Facial Gestures in
                            Dyadic Settings</a></h5>

                    <ul class="rlist--inline loa truncate-list"
                        title="list of authors" data-lines="2">
                        <li><a href="https://dl.acm.org/profile/99658697077"
                                title="Patrik Jonell"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Patrik Jonell profile image" /><span>Patrik
                                    Jonell</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99658697077">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/99659309831"
                                title="Taras Kucherenko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/do/10.1145/contrib-99659309831/rel-imgonly/taras.jpg"
                                    alt="Taras Kucherenko profile image" /><span>Taras
                                    Kucherenko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659309831">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81553344056"
                                title="Gustav Eje Henter"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Gustav Eje Henter profile image" /><span>Gustav
                                    Eje Henter</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81553344056">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81100428426"
                                title="Jonas Beskow"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Jonas Beskow profile image" /><span>Jonas
                                    Beskow</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81100428426">KTH
                                    Royal Institute of Technology</p>
                            </span></li>
                    </ul>

                    <div class="issue-item__detail"><span>October
                            2020</span><span class="dot-separator">pp 1-8
                        </span><span><a
                                href="https://doi.org/10.1145/3383652.3423911"
                                class="issue-item__doi  dot-separator">https://doi.org/10.1145/3383652.3423911</a></span>
                    </div>
                    <div data-lines='4'
                        class="issue-item__abstract truncate-text">
                        <div class="issue-item__abstract truncate-text"
                            data-lines="4">

                            <p>To enable more natural face-to-face interactions,
                                conversational agents need to adapt
                                their behavior to their interlocutors. One key
                                aspect of this is generation of appropriate
                                non-verbal behavior for the agent, for example
                                facial gestures, here defined ...
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </li>
        <li class="grid-item separated-block--dashed--bottom">
            <div class="issue-item clearfix">
                <div class="issue-item__citation">
                    <div class="issue-heading">extended-abstract</div>
                </div>
                <div class="issue-item__content">
                    <h5 class="issue-item__title"><a
                            href="https://dl.acm.org/doi/10.1145/3383652.3423874?cid=99659309831">Generating
                            coherent spontaneous speech and gesture from
                            text</a></h5>

                    <ul class="rlist--inline loa truncate-list"
                        title="list of authors" data-lines="2">
                        <li><a href="https://dl.acm.org/profile/86159065557"
                                title="Simon Alexanderson"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Simon Alexanderson profile image" /><span>Simon
                                    Alexanderson</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-86159065557">KTH
                                    Royal Institute of Technology, Stockholm,
                                    Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81548864956"
                                title="Éva Székely"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Éva Székely profile image" /><span>Éva
                                    Székely</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81548864956">KTH
                                    Royal Institute of Technology, Stockholm,
                                    Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81553344056"
                                title="Gustav Eje Henter"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Gustav Eje Henter profile image" /><span>Gustav
                                    Eje Henter</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81553344056">KTH
                                    Royal Institute of Technology, Stockholm,
                                    Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/99659309831"
                                title="Taras Kucherenko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/do/10.1145/contrib-99659309831/rel-imgonly/taras.jpg"
                                    alt="Taras Kucherenko profile image" /><span>Taras
                                    Kucherenko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659309831">KTH
                                    Royal Institute of Technology, Stockholm,
                                    Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81100428426"
                                title="Jonas Beskow"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Jonas Beskow profile image" /><span>Jonas
                                    Beskow</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81100428426">KTH
                                    Royal Institute of Technology, Stockholm,
                                    Sweden</p>
                            </span></li>
                    </ul>

                    <div class="issue-item__detail"><span>October
                            2020</span><span class="dot-separator">pp 1-3
                        </span><span><a
                                href="https://doi.org/10.1145/3383652.3423874"
                                class="issue-item__doi  dot-separator">https://doi.org/10.1145/3383652.3423874</a></span>
                    </div>
                    <div data-lines='4'
                        class="issue-item__abstract truncate-text">
                        <div class="issue-item__abstract truncate-text"
                            data-lines="4">

                            <p>Embodied human communication encompasses both
                                verbal (speech) and non-verbal information
                                (e.g., gesture and head movements). Recent
                                advances in machine learning have substantially
                                improved the technologies for generating
                                synthetic versions of both of ...
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </li>
        <li class="grid-item separated-block--dashed--bottom">
            <div class="issue-item clearfix">
                <div class="issue-item__citation">
                    <div class="issue-heading">research-article</div>
                </div>
                <div class="issue-item__content">
                    <h5 class="issue-item__title"><a
                            href="https://dl.acm.org/doi/10.1145/3383652.3423860?cid=99659309831">Can
                            we trust online crowdworkers?: Comparing online and
                            offline participants in a preference test of virtual
                            agents</a></h5>

                    <ul class="rlist--inline loa truncate-list"
                        title="list of authors" data-lines="2">
                        <li><a href="https://dl.acm.org/profile/99658697077"
                                title="Patrik Jonell"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Patrik Jonell profile image" /><span>Patrik
                                    Jonell</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99658697077">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/99659309831"
                                title="Taras Kucherenko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/do/10.1145/contrib-99659309831/rel-imgonly/taras.jpg"
                                    alt="Taras Kucherenko profile image" /><span>Taras
                                    Kucherenko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659309831">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/99659460087"
                                title="Ilaria Torre"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Ilaria Torre profile image" /><span>Ilaria
                                    Torre</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659460087">KTH
                                    Royal Institute of Technology</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81100428426"
                                title="Jonas Beskow"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Jonas Beskow profile image" /><span>Jonas
                                    Beskow</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81100428426">KTH
                                    Royal Institute of Technology</p>
                            </span></li>
                    </ul>

                    <div class="issue-item__detail"><span>October
                            2020</span><span class="dot-separator">pp 1-8
                        </span><span><a
                                href="https://doi.org/10.1145/3383652.3423860"
                                class="issue-item__doi  dot-separator">https://doi.org/10.1145/3383652.3423860</a></span>
                    </div>
                    <div data-lines='4'
                        class="issue-item__abstract truncate-text">
                        <div class="issue-item__abstract truncate-text"
                            data-lines="4">

                            <p>Conducting user studies is a crucial component in
                                many scientific fields. While some
                                studies require participants to be physically
                                present, other studies can be conducted
                                both physically (e.g. in-lab) and online (e.g.
                                via crowdsourcing). Inviting ...
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </li>
        <li class="grid-item separated-block--dashed--bottom">
            <div class="issue-item clearfix">
                <div class="issue-item__citation">
                    <div class="issue-heading">research-article</div>
                </div>
                <div class="issue-item__content">
                    <h5 class="issue-item__title"><a
                            href="https://dl.acm.org/doi/10.1145/3308532.3329472?cid=99659309831">Analyzing
                            Input and Output Representations for Speech-Driven
                            Gesture Generation</a></h5>

                    <ul class="rlist--inline loa truncate-list"
                        title="list of authors" data-lines="2">
                        <li><a href="https://dl.acm.org/profile/99659309831"
                                title="Taras Kucherenko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/do/10.1145/contrib-99659309831/rel-imgonly/taras.jpg"
                                    alt="Taras Kucherenko profile image" /><span>Taras
                                    Kucherenko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659309831">KTH
                                    Royal Institute of Technology in Stockholm,
                                    Stockholm, Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81460641491"
                                title="Dai Hasegawa"><img class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Dai Hasegawa profile image" /><span>Dai
                                    Hasegawa</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81460641491">Hokkai
                                    Gakuen University, Sapporo, Japan</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81553344056"
                                title="Gustav Eje Henter"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Gustav Eje Henter profile image" /><span>Gustav
                                    Eje Henter</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81553344056">KTH
                                    Royal Institute of Technology in Stockholm,
                                    Stockholm, Sweden</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/99659212318"
                                title="Naoshi Kaneko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Naoshi Kaneko profile image" /><span>Naoshi
                                    Kaneko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659212318">Aoyama
                                    Gakuin University, Tokyo, Japan</p>
                            </span><span>, </span></li>
                        <li><a href="https://dl.acm.org/profile/81100031743"
                                title="Hedvig Kjellström"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg"
                                    alt="Hedvig Kjellström profile image" /><span>Hedvig
                                    Kjellström</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-81100031743">KTH
                                    Royal Institute of Technology in Stockholm,
                                    Stockholm, Sweden</p>
                            </span></li>
                    </ul>

                    <div class="issue-item__detail"><span>July 2019</span><span
                            class="dot-separator">pp 97-104 </span><span><a
                                href="https://doi.org/10.1145/3308532.3329472"
                                class="issue-item__doi  dot-separator">https://doi.org/10.1145/3308532.3329472</a></span>
                    </div>
                    <div data-lines='4'
                        class="issue-item__abstract truncate-text">
                        <div class="issue-item__abstract truncate-text"
                            data-lines="4">

                            <p>This paper presents a novel framework for
                                automatic speech-driven gesture generation,
                                applicable to human-agent interaction including
                                both virtual agents and robots. Specifically,
                                we extend recent deep-learning-based,
                                data-driven methods for speech-...
                            </p>
                            <p></p>
                            <p></p>
                        </div>
                    </div>
                </div>
            </div>
        </li>
        <li class="grid-item separated-block--dashed--bottom">
            <div class="issue-item clearfix">
                <div class="issue-item__citation">
                    <div class="issue-heading">research-article</div>
                </div>
                <div class="issue-item__content">
                    <h5 class="issue-item__title"><a
                            href="https://dl.acm.org/doi/10.1145/3242969.3264970?cid=99659309831">Data
                            Driven Non-Verbal Behavior Generation for Humanoid
                            Robots</a></h5>

                    <ul class="rlist--inline loa truncate-list"
                        title="list of authors" data-lines="2">
                        <li><a href="https://dl.acm.org/profile/99659309831"
                                title="Taras Kucherenko"><img
                                    class="author-picture"
                                    src="https://dl.acm.org/do/10.1145/contrib-99659309831/rel-imgonly/taras.jpg"
                                    alt="Taras Kucherenko profile image" /><span>Taras
                                    Kucherenko</span></a><span
                                class="loa_author_inst hidden">
                                <p data-doi="10.1145/contrib-99659309831">KTH
                                    Royal Institute of Technology in Stockholm,
                                    Stockholm, Sweden</p>
                            </span></li>
                    </ul>

                    <div class="issue-item__detail"><span>October
                            2018</span><span class="dot-separator">pp 520-523
                        </span><span><a
                                href="https://doi.org/10.1145/3242969.3264970"
                                class="issue-item__doi  dot-separator">https://doi.org/10.1145/3242969.3264970</a></span>
                    </div>
                    <div data-lines='4'
                        class="issue-item__abstract truncate-text">
                        <div class="issue-item__abstract truncate-text"
                            data-lines="4">

                            <p>Social robots need non-verbal behavior to make an
                                interaction pleasant and efficient.
                                Most of the models for generating non-verbal
                                behavior are rule-based and hence can
                                produce a limited set of motions and are tuned
                                to a particular scenario. In contrast,...
                            </p>
                            <p></p>
                        </div>
                    </div>
                </div>
            </div>
        </li>
    </ul>
</div>
<link rel="stylesheet"
    href="https://dl.acm.org/specs/products/acm/widgets/authorizer/scss/style.css" />
