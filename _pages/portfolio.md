---
layout: page
title: Videos
permalink: /portfolio/
---

### 2022

* Youngwoo Yoon\*, Pieter Wolfert\*, **Taras Kucherenko**\*, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter. *The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation*. In Proceedings of the ACM International Conference on Multimodal Interaction (ICMI). 2022

<iframe width="560" height="315" src="https://www.youtube.com/embed/4n02wXGGnd0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### 2021

Rajmund Nagy\*, Taras Kucherenko\*, Birger Moell, André Pereira, Hedvig Kjellström, and Ulysses Bernardet. *A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents.* 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS). 2021

<iframe width="560" height="315" src="https://www.youtube.com/embed/jhgUBS0125A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Taras Kucherenko\*, Patrik Jonell\*, Youngwoo Yoon\*, Pieter Wolfert, and Gustav Eje Henter. *A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020.* International Conference on Intelligent User Interfaces. 2021


<iframe width="560" height="315" src="https://www.youtube.com/embed/ja7IXGFrYGA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

### 2020

Taras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, Simon Alexanderson, Iolanda Leite, and Hedvig Kjellström. *Gesticulator: A framework for semantically-aware speech-driven gesture generation.* International Conference on Multimodal Interaction (ICMI ‘20) 

**Best Paper Award**
<iframe width="560" height="315" src="https://www.youtube.com/embed/VQ8he6jjW08" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow. *Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings*. International Conference on Intelligent Virtual Agents (IVA'20). 2020.

**Best Paper Award**
<iframe width="560" height="315" src="https://www.youtube.com/embed/RhazMS4L_bk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Simon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. *Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows.* EuroGraphics 2020 

**Best Paper Award Nominee**

<iframe width="560" height="315" src="https://www.youtube.com/embed/egf3tjbWBQE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

### 2019

A general overview of my research.
<iframe width="560" height="315" src="https://www.youtube.com/embed/AS5VorjTwcg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Taras  Kucherenko,  Dai  Hasegawa, Gustav  Eje  Henter, Naoshi  Kaneko, and Hedvig Kjellström. 
[Analyzing input and output representations for speech-driven gesture generation](https://www.researchgate.net/publication/331645229_Analyzing_Input_and_Output_Representations_for_Speech-Driven_Gesture_Generation).
International Conference on Intelligent Virtual Agents (IVA '19), Paris, July 02–05, 2019

Code is publicly available at [Github](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder)

<iframe width="560" height="315" src="https://www.youtube.com/embed/Iv7UBe92zrw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Below is a demo applying that model to a new dataset (which is in English).
To reproduce the results you can use our [pre-trained model](https://github.com/Svito-zar/speech-driven-hand-gesture-generation-demo)

<iframe width="560" height="315" src="https://youtube.com/embed/tQLVyTVtsSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### 2018

Taras  Kucherenko, Jonas Beskow and Hedvig Kjellström. 
[A neural network approach to missing marker reconstruction in human motion capture.](https://arxiv.org/abs/1803.02665)
arXiv preprint arXiv:1803.02665 (2018).

<iframe width="560" height="315" src="https://www.youtube.com/embed/mi75gzEhbHI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The system from above could also do Denoising.
Code is publicly available at [Github](https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction)

<iframe width="560" height="315" src="https://www.youtube.com/embed/MFdFqxCNhN0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



