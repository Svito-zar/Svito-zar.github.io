---
layout: page
title: Videos
permalink: /portfolio/
---

### 2020

Taras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, Simon Alexanderson, Iolanda Leite, and Hedvig Kjellström. *Gesticulator: A framework for semantically-aware speech-driven gesture generation.* International Conference on Multimodal Interaction (ICMI ‘20) 

**Best Paper Award Nominee**
<iframe width="560" height="315" src="https://www.youtube.com/embed/VQ8he6jjW08" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow. *Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings*. International Conference on Intelligent Virtual Agents (IVA'20). 2020.

**Best Paper Award**
<iframe width="560" height="315" src="https://www.youtube.com/embed/RhazMS4L_bk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Simon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. *Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows.* EuroGraphics 2020 

**Best Paper Award Nominee**

<iframe width="560" height="315" src="https://www.youtube.com/embed/egf3tjbWBQE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### 2019

A general overview of my research.
<iframe width="560" height="315" src="https://www.youtube.com/embed/AS5VorjTwcg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

&nbsp;

***
&nbsp;

Taras  Kucherenko,  Dai  Hasegawa, Gustav  Eje  Henter, Naoshi  Kaneko, and Hedvig Kjellström. 
[Analyzing input and output representations for speech-driven gesture generation](https://www.researchgate.net/publication/331645229_Analyzing_Input_and_Output_Representations_for_Speech-Driven_Gesture_Generation).
International Conference on Intelligent Virtual Agents (IVA '19), Paris, July 02–05, 2019

Code is publicly available at [Github](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder)

<iframe width="560" height="315" src="https://www.youtube.com/embed/Iv7UBe92zrw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Below is a demo applying that model to a new dataset (which is in English).
To reproduce the results you can use our [pre-trained model](https://github.com/Svito-zar/speech-driven-hand-gesture-generation-demo)

<iframe width="560" height="315" src="https://youtube.com/embed/tQLVyTVtsSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### 2018

Taras  Kucherenko, Jonas Beskow and Hedvig Kjellström. 
[A neural network approach to missing marker reconstruction in human motion capture.](https://arxiv.org/abs/1803.02665)
arXiv preprint arXiv:1803.02665 (2018).

<iframe width="560" height="315" src="https://www.youtube.com/embed/mi75gzEhbHI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The system from above could also do Denoising.
Code is publicly available at [Github](https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction)

<iframe width="560" height="315" src="https://www.youtube.com/embed/MFdFqxCNhN0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



