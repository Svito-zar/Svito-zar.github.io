---
layout: page
title: Audio2Gestures
permalink: /audio2gestures/
---

# Analyzing input and output representations
# for speech-driven gesture generation
## Taras  Kucherenko,  Dai  Hasegawa, Gustav  Eje  Henter, Naoshi  Kaneko, and Hedvig Kjellstr√∂m. 
### International Conference on Intelligent Virtual Agents (IVA '19)

[Paper](https://www.researchgate.net/publication/331645229_Analyzing_Input_and_Output_Representations_for_Speech-Driven_Gesture_Generation) [Code](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder).


Code is publicly available at [Github](https://github.com/GestureGeneration/Speech_driven_gesture_generation_with_autoencoder)

<iframe width="560" height="315" src="https://www.youtube.com/embed/Iv7UBe92zrw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Below is a demo applying that model to a new dataset (which is in English).
To reproduce the results you can use our [pre-trained model](https://github.com/Svito-zar/speech-driven-hand-gesture-generation-demo)

<iframe width="560" height="315" src="https://youtube.com/embed/tQLVyTVtsSU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

