---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: home
title: 
exclude: true
---



<img style="float: left; border: 5px solid white; padding-right: 10px;" src="assets/Prof_pic.jpg" height="230" alt="portrait">
I am a Research Scientist at EA Games. Recently I finished my PhD at KTH Royal Institute of Technology in Stockholm. My main supervisor was [Hedvig Kjellström](http://www.csc.kth.se/~hedvig/) and co-supervisors were [Gustav Eje Henter](https://people.kth.se/~ghe/), Jonas Beskow and [Iolanda Leite](https://iolandaleite.com/).

My research is on machine learning models for non-verbal behavior generation, such as hand gestures and facial expressions. One of my talks about it you can find [in this video](https://youtu.be/JeMwtr8pxcc). My favorite project, GENEA Challenge 2020, can be found on [this project page](https://svito-zar.github.io/GENEAchallenge2020/). My thesis is publicly available at [this url](https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1609615&dswid=9110).



<br>
### **Awards**

* <strong>Oct 2021:</strong> I have got the [ICMI 2021](https://icmi.acm.org/2021/) Best Reviewer Award!
* <strong>Sept 2021:</strong> Our paper [Speech2Properties2Gestures](https://dl.acm.org/doi/10.1145/3472306.3478333?cid=99659309831) received Honourable Mention award at [IVA 2021](https://sites.google.com/view/iva2021/).
* <strong>Oct 2020:</strong> [Gesticulator](https://svito-zar.github.io/gesticulator/) have got the ICMI 2020 Best Paper Award!
* <strong>Oct 2020:</strong> [Let's face it](https://jonepatr.github.io/lets_face_it/) received the IVA 2020 Best Paper Award!
* <strong>May 2020:</strong> We received Honourable Mention award at Eurographics 2020 for our paper [Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows](https://diglib.eg.org/handle/10.1111/cgf13946).

<br>
### **Other news**

* <strong>Feb 2022:</strong> I joined EA Games R&D department, called SEED, as research engineer.
* <strong>Dec 2021:</strong> Our paper [Multimodal analysis of the predictability of hand-gesture properties](https://arxiv.org/abs/2108.05762) was accepted to [AAMAS 2022](https://aamas2022-conference.auckland.ac.nz/).
* <strong>Dec 2021:</strong> I defended my thesis. My opponent was Stefan Kopp and you can find more info under [this link](https://svito-zar.github.io/defence/)
* <strong>Nov 2021</strong> I gave two talks about my research on co-speech gesture synthesis: for [Social AI and Robotics (SAIR) Lab](https://sairlab.github.io/) at [King's College London](https://www.kcl.ac.uk/) and for [Affective Intelligence and Robotics Laboratory](https://cambridge-afar.github.io/) at [Cambridge University](https://www.cam.ac.uk/)
* <strong>Oct 2021:</strong> Our GENEA (Generation and Evaluation of Non-verbal Behavior for Embodied Agents) Workshop 2021 is happening at ICMI 2021. [Workshop page](https://genea-workshop.github.io/2021/)
* <strong>Sept 2021:</strong> Started an internship at Microsoft Research UK Applied Research team
* <strong>July 2021:</strong> Two papers accepted at [ICMI 2021](https://icmi.acm.org/2021/) : [HEMVIP: Human Evaluation of Multiple Videos in Parallel](https://arxiv.org/abs/2101.11898) and [To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures](https://arxiv.org/abs/2108.05709).
* <strong>June 2021:</strong> Our paper [Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech](https://dl.acm.org/doi/pdf/10.1145/3472306.3478333) was accepted to [IVA 2021](https://sites.google.com/view/iva2021/).
* <strong>Mar 2021:</strong> Our GENEA (Generation and Evaluation of Non-verbal Behavior for Embodied Agents) Workshop 2021 has been accepted as an official workshop at ICMI 2021. [Call for papers](https://genea-workshop.github.io/2021/#call-for-papers) is out!
* <strong>Feb 2021:</strong> Our demonstrator [A Framework for Integrating Gesture Generation Models into Interactive Conversational Agents](http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1779.pdf) was accepted to the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021).
* <strong>Feb 2021:</strong> [Synced](https://syncedreview.com/) wrote an article about [Gesticulator](https://svito-zar.github.io/gesticulator/), which you can access under [this link](https://syncedreview.com/2021/02/10/icmi-2020-best-paper-gesticulator-a-framework-for-semantically-aware-speech-driven-gesture-generation/).
* <strong>Jan 2021:</strong> Our paper [Moving fast and slow: Analysis of representations and post-processing in speech-driven automatic gesture generation](https://www.tandfonline.com/doi/full/10.1080/10447318.2021.1883883) got accepted to the International Journal of Human-Computer Interaction.
* <strong>Dec 2020:</strong> Our paper [A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020](https://dl.acm.org/doi/10.1145/3397481.3450692) was accepted to IUI 2021 conference.
* <strong>Oct 2020:</strong> I will give a talk about my work on gesture generation for the [Talking Robotics](https://talking-robotics.github.io) seminar series on October 30th 4pm UTC(GMT). More details [under this link](https://talking-robotics.github.io/session_details/taras.html).
* <strong>Sept 2020:</strong> I have an open position for a master thesis on benchmarking gesture generation models in an interaction. See [the proposal](https://www.kth.se/profile/tarask/page/master-thesis-proposal) for more details.
* <strong>Sept 2020:</strong> One more paper accepted to IVA 2020: [Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings](https://arxiv.org/abs/2006.09888).
* <strong>Aug 2020:</strong> Our paper [Gesticulator: A framework for semantically-aware speech-driven gesture generation](https://arxiv.org/abs/2001.09326) got accepted to ICMI 2020!
* <strong>July 2020:</strong> Two papers accepted for [IVA 2020](http://iva2020.psy.gla.ac.uk). More details in the [publications](https://svito-zar.github.io/publications/) tap.
* <strong> April 2020: </strong> The CFP for our IVA'20 [Workshop](https://genea-workshop.github.io/2020/) on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents is [out](https://easychair.org/cfp/GENEA_Workshop_2020). As part of this workshop we organize Gesture Generation Challenge!
* <strong>Feb 2020:</strong> Our paper [Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows](https://diglib.eg.org/handle/10.1111/cgf13946) was accepted to Eurographics 2020.
* <strong>Jan 2020:</strong> Sifan Jiang started his master thesis with me. He will be extending my gesture generation model to humanoid robot NAO.
* <strong>November 2019:</strong> [Talk](https://youtu.be/AS5VorjTwcg) on general direction of my research.
* <strong>October 2019:</strong> Master students wanted for a master thesis project on gesture generation for a humanoid robot.
* <strong>September 2019</strong> Talk on [How to make your agent gesture in a natural way](https://ps.is.tuebingen.mpg.de/events/how-to-make-your-agent-gesture-in-a-natural-way) at Max Planck Institute for Intelligent Systems: [Perceiving Systems](https://ps.is.tuebingen.mpg.de) Department in Tuebingen.
* <strong>August 2019:</strong> Our gesture generation model was applied to a new dataset. Now it can gesticulate in both Japanese and English. Check out a short [demo video](https://youtu.be/tQLVyTVtsSU) and our [code](https://github.com/Svito-zar/speech-driven-hand-gesture-generation-demo) with a pre-trained model.
* <strong>June 2019:</strong> Our two papers were accepted for [ICDL-EPIROB 2019 Workshop](https://nicolas-navarro-guerrero.gitlab.io/workshop-non-verbal-human-robot-interactions-icdl-epirob-2019/) on Naturalistic Non-Verbal and Affective Human-Robot Interactions.
* <strong>May 2019:</strong> Talk on my research at the [Pint Of Science](http://pintofscience.se/) in Stockholm.
* <strong>April 2019:</strong> Our paper [Analyzing input and output representations for speech-driven gesture generation](https://www.researchgate.net/publication/331645229_Analyzing_Input_and_Output_Representations_for_Speech-Driven_Gesture_Generation) was accepter to [IVA 2019](https://iva2019.sciencesconf.org/) for oral presentation. (24% acceptance rate)
* <strong>Jan 2019:</strong> Our paper [On the importance of representations for speech-driven gesture generation](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p2072.pdf) was accepted at [AAMAS 2019](http://aamas2019.encs.concordia.ca/) for poster presentation.
* <strong>Oct 2018:</strong> My [project proposal](https://www.researchgate.net/publication/328032360_Data_Driven_Non-Verbal_Behavior_Generation_for_Humanoid_Robots) was published ICMI Doctoral Consortium 2018.
* <strong>April 2018:</strong> Joined Social Robotics Sweden ([SoRoS](https://soros-community.github.io/)) community.
* <strong>June 2017:</strong> We had a [poster](https://www.csc.kth.se/~hedvig/publications/ssdl_17.pdf) at The First Swedish Symposium on Deep Learning (SSDL).

&nbsp;
&nbsp;

### **Academic Service**

* <strong> Co-organizing </strong>
    - ICMI'21 [Workshop](https://genea-workshop.github.io/2021/) on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents
    - IVA'20 [Workshop](https://genea-workshop.github.io/2020/) on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents
    - [GESPIN 2020 Conference](http://sprakbanken.speech.kth.se/events/gespin/)
    - [SoRoS](https://soros-community.github.io/)  2020 Workshop
    - NIPS 2018 [Workshop on AI for Social Good](https://aiforsocialgood.github.io/2018/cfp.htm)


* <strong> Reviewer </strong> for
    - HRI 2022, CHI 2022, CVPR 2022, AISTATS 2022
    - ACII 2021, ICMI 2021, ROMAN 2021, ICCV 2021 Workshops
    - ECAI 2020, IJCAI 2020, SIGRAPH 2020, IVA 2020, ICMI 2020 LBR
    - ACII 2019, ICSR 2019
    - NIPS 2018 Workshop on AI for Social Good




