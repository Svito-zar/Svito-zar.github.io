---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: home
title: 
exclude: true
---


<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />
<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">


<img style="float: left; border: 5px solid white; padding-right: 10px;" src="assets/Prof_pic.jpg" height="230" alt="portrait">
I am a Ph.D. student in Machine Learning for Social Robotics at KTH Royal Institute of Technology in Stockholm. My main supervisor is [Hedvig Kjellström](http://www.csc.kth.se/~hedvig/) and co-supervisors are [Gustav Eje Henter](https://people.kth.se/~ghe/) and [Iolanda Leite](https://iolandaleite.com/).

My research is on machine learning models for non-verbal behavior generation, such as hand gestures and facial expressions. I recently gave a talk about it, which you find [in this video](https://youtu.be/JeMwtr8pxcc). My latest project, Gesticulator, can be found on [this project page](https://svito-zar.github.io/gesticulator).

I am working in the HealthTech project [EACare](http://www.csc.kth.se/cvap/EACare/), where we aim to develop a robot system to detect early signs of Dementia from the communicative behavior. My part is non-verbal behavior generation.




<br>
### **News**
* <strong>Feb 2021:</strong> [Synced](https://syncedreview.com/) wrote an article about [Gesticulator](https://svito-zar.github.io/gesticulator/), which you can access under [this link](https://syncedreview.com/2021/02/10/icmi-2020-best-paper-gesticulator-a-framework-for-semantically-aware-speech-driven-gesture-generation/)
* <strong>Jan 2021:</strong> Our paper [Moving fast and slow: Analysis of representations and post-processing in speech-driven automatic gesture generation](https://www.tandfonline.com/doi/full/10.1080/10447318.2021.1883883) got accepted to the International Journal of Human-Computer Interaction.
* <strong>Dec 2020:</strong> Our paper [A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020](https://arxiv.org/abs/2102.11617) was accepted to IUI 2021 conference.
* <strong>Oct 2020:</strong> [Gesticulator](https://svito-zar.github.io/gesticulator/) have got the ICMI 2020 Best Paper Award!
* <strong>Oct 2020:</strong> [Let's face it](https://jonepatr.github.io/lets_face_it/) received the IVA 2020 Best Paper Award!
* <strong>Oct 2020:</strong> I will give a talk about my work on gesture generation for the [Talking Robotics](https://talking-robotics.github.io) seminar series on October 30th 4pm UTC(GMT). More details [under this link](https://talking-robotics.github.io/session_details/taras.html).
* <strong>Sept 2020:</strong> I have an open position for a master thesis on benchmarking gesture generation models in an interaction. See [the proposal](https://www.kth.se/profile/tarask/page/master-thesis-proposal) for more details.
* <strong>Sept 2020:</strong> One more paper accepted to IVA 2020: [Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings](https://arxiv.org/abs/2006.09888).
* <strong>Aug 2020:</strong> Our paper [Gesticulator: A framework for semantically-aware speech-driven gesture generation](https://arxiv.org/abs/2001.09326) got accepted to ICMI 2020!
* <strong>July 2020:</strong> Two papers accepted for [IVA 2020](http://iva2020.psy.gla.ac.uk). More details in the [publications](https://svito-zar.github.io/publications/) tap.
* <strong>May 2020:</strong> We received honourable mention award at Eurographics 2020 for our paper [Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows](https://diglib.eg.org/handle/10.1111/cgf13946).
* <strong> April 2020: </strong> The CFP for our IVA'20 [Workshop](https://genea-workshop.github.io/2020/) on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents is [out](https://easychair.org/cfp/GENEA_Workshop_2020). As part of this workshop we organize Gesture Generation Challenge!
* <strong>Feb 2020:</strong> Our paper [Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows](https://diglib.eg.org/handle/10.1111/cgf13946) was accepted to Eurographics 2020.
* <strong>Jan 2020:</strong> Sifan Jiang started his master thesis with me. He will be extending my gesture generation model to humanoid robot NAO.
* <strong>November 2019:</strong> [Talk](https://youtu.be/AS5VorjTwcg) on general direction of my research.
* <strong>October 2019:</strong> Master students wanted for a master thesis project on gesture generation for a humanoid robot.
* <strong>September 2019</strong> Talk on [How to make your agent gesture in a natural way](https://ps.is.tuebingen.mpg.de/events/how-to-make-your-agent-gesture-in-a-natural-way) at Max Planck Institute for Intelligent Systems: [Perceiving Systems](https://ps.is.tuebingen.mpg.de) Department in Tuebingen.
* <strong>August 2019:</strong> Our gesture generation model was applied to a new dataset. Now it can gesticulate in both Japanese and English. Check out a short [demo video](https://youtu.be/tQLVyTVtsSU) and our [code](https://github.com/Svito-zar/speech-driven-hand-gesture-generation-demo) with a pre-trained model.
* <strong>June 2019:</strong> Our two papers were accepted for [ICDL-EPIROB 2019 Workshop](https://nicolas-navarro-guerrero.gitlab.io/workshop-non-verbal-human-robot-interactions-icdl-epirob-2019/) on Naturalistic Non-Verbal and Affective Human-Robot Interactions.
* <strong>May 2019:</strong> Talk on my research at the [Pint Of Science](http://pintofscience.se/) in Stockholm.
* <strong>April 2019:</strong> Our paper [Analyzing input and output representations for speech-driven gesture generation](https://www.researchgate.net/publication/331645229_Analyzing_Input_and_Output_Representations_for_Speech-Driven_Gesture_Generation) was accepter to [IVA 2019](https://iva2019.sciencesconf.org/) for oral presentation. (24% acceptance rate)
* <strong>Jan 2019:</strong> Our paper [On the importance of representations for speech-driven gesture generation](http://www.ifaamas.org/Proceedings/aamas2019/pdfs/p2072.pdf) was accepted at [AAMAS 2019](http://aamas2019.encs.concordia.ca/) for poster presentation.
* <strong>Oct 2018:</strong> My [project proposal](https://www.researchgate.net/publication/328032360_Data_Driven_Non-Verbal_Behavior_Generation_for_Humanoid_Robots) was published ICMI Doctoral Consortium 2018.
* <strong>April 2018:</strong> Joined Social Robotics Sweden ([SoRoS](https://soros-community.github.io/)) community.
* <strong>June 2017:</strong> We had a [poster](https://www.csc.kth.se/~hedvig/publications/ssdl_17.pdf) at The First Swedish Symposium on Deep Learning (SSDL).

&nbsp;
&nbsp;

### **Academic Service**

* <strong> Co-organizing </strong>
    - IVA'20 [Workshop](https://genea-workshop.github.io/2020/) on Generation and Evaluation of Non-verbal Behaviour for Embodied Agents
    - [GESPIN 2020 Conference](http://sprakbanken.speech.kth.se/events/gespin/)
    - [SoRoS](https://soros-community.github.io/)  2020 Workshop
    - NIPS 2018 [Workshop on AI for Social Good](https://aiforsocialgood.github.io/2018/cfp.htm)


* <strong> Reviewer </strong> for
	* ECAI 2020, IJCAI 2020, SIGRAPH 2020, IVA 2020, ICMI 2020 LBR
	* ACII 2019, ICSR 2019
	* NIPS 2018 Workshop on AI for Social Good




